{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs - Data Wrangling Summary\n",
    "\n",
    "*Greg Clunies*<br>\n",
    "*06/16/2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Background\n",
    "\n",
    "This report discusses the process of wrangling various sources of data related to the [WeRateDogs](https://twitter.com/dog_rates) Twitter account.\n",
    "\n",
    "Three sources of data were wrangle during the effort, listed below:\n",
    "\n",
    "1. `twitter_archive_enhanced.csv` - archived data from WeRateDogs. Provided as a standalone file.\n",
    "2. `image_predictions.tsv` - Tweet image predictions results from a neural network. Downloaded programmatcially from https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "3. Retweet and favorite count data retrieved via the Twitter API. More info on the Twitter API can be found [here](https://developer.twitter.com/en/docs.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling Process\n",
    "\n",
    "Data wrangling can be split up into three primary activities:\n",
    "\n",
    "1. Gathering data: Pulling data from various sources into our working environmen for further use in assessment and cleaning stages.\n",
    "2. Assessing data: reviewing the data's structure and determining if their are content (Completeness, Validity, Accuracy, Consistency... aka Dirty data) or strutural issues (is the data [Tidy](https://vita.had.co.nz/papers/tidy-data.pdf)?) with the data.\n",
    "3. Cleaning the data programmatically to address the issues identified during assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Gather \n",
    "\n",
    "As stated earlier, 3 data sources were gathered as part of the wrnagling effort.\n",
    "\n",
    "1. `twitter_archive_enhanced.csv` was read into a jupyter notebook `wrangle_act.ipynb` using pandas' `read_csv` function. The end result is a pandas dataframe `twitter_archive`.\n",
    "\n",
    "2. Image predictions results from a neural network, was downloaded programmatcially using the requests library from the URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.ts. The response of `r = requests.get(url)` was written to a file `image_predictions.tsv`. This file was then converted to a pandas dataframe,`image_predictions`, using pandas' `read_csv` function again using the code below:<br><br>\n",
    "```image_predictions = pd.read_csv('image_predictions.tsv', sep='\\t')```\n",
    "\n",
    "3. Additional tweet information (retweet count and favorite count) were obtained using the Twitter API for each twet based on thier tweet ID. This additional data for each tweet was written to the file `tweet_json.txt` (one line per tweet status) in JSON format. This file was then read line by line using a for loop and the relevant data extraacted from each entry using the `json` library. This data was then stored in a new dataframe, `df-tweets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Assess\n",
    "\n",
    "After the data was gathered, it was assessed for both Quality and Tidyness. A summary of the findings for each is shown below, organized by the three data sources.\n",
    "\n",
    "### Quality\n",
    "\n",
    "##### **`twitter_archive`**\n",
    "- Some tweets where the dog name is not listed in tweet have a `name` entry of '`a`' instead of None.\n",
    "- `tweet_id` is of type float, not string.\n",
    "- `timestamp` is a string, not at datetime object.\n",
    "- There are 78 retweets included in the dataset.\n",
    "- Some `text` entries contain strange characters after '&'\n",
    "- Some tweets have no photo (e.g., `tweet_id` = 840698636975636481, has no `expanded_url`)\n",
    "\n",
    "##### **`image_predictions`**\n",
    "- `tweet_id` is int, not string.\n",
    "- `image_predictions` only for 2075 tweets, but `twitter_archive` contains 2356 tweets (78 are retweets)\n",
    "\n",
    "##### **`df_tweets`**\n",
    "- `tweet_id` is int, not string.\n",
    "\n",
    "### Tidyness\n",
    "\n",
    "##### **`twitter_archive`**\n",
    "- doggo, floofer, pupper, puppo columns should be one column named \"dog_type\"\n",
    "\n",
    "##### General Tidyness \n",
    "- `twitter_archive`, `image_predictions`, `df_tweets` should be joined as one table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Cleaning\n",
    "\n",
    "Using the issues defined above during the Assess stage. Each issue was resolved programmatically using a **Define, Code, Test** approach to ensure each individual cleaning effort was clearly identified, implemented, and checked. This resulted in three cleaned data frames: \n",
    "- `twitter_archive_clean`\n",
    "- `image_predictions_clean`\n",
    "- `df_tweets`\n",
    "\n",
    "The final step in the cleaning process was to join all three cleaned dataframes to a master dataframe, `df_master`, using pandas merge function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Wrangling\n",
    "\n",
    "While the wrangle process may never be complete, due to it's iterative nature, the master dataset `df_master` is now much more manageble for easy exploration. We can now move onto analysis through visualization and/or statistical methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
